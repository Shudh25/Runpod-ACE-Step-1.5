# =============================================================================
# ACE-Step 1.5 - RunPod Serverless TRAINING Worker
# Target GPU: A100 80GB
# Output: RunPod Network Volume only
# =============================================================================

# -----------------------------------------------------------------------------
# Stage 1: Model Downloader
# Only base DiT + VAE needed — no LM, no turbo
# -----------------------------------------------------------------------------
FROM python:3.11-slim AS model-downloader

ARG HF_TOKEN
ENV HF_TOKEN=${HF_TOKEN}

WORKDIR /models

RUN pip install --no-cache-dir "huggingface-hub[cli,hf_transfer]"
ENV HF_HUB_ENABLE_HF_TRANSFER=1

# Main bundle: VAE + Qwen3-Embedding text encoder
# Skip LM — not needed for training
RUN python -c "import os; from huggingface_hub import snapshot_download; snapshot_download('ACE-Step/Ace-Step1.5', local_dir='/models/checkpoints', token=os.environ.get('HF_TOKEN'), ignore_patterns=['acestep-v15-turbo/*', 'acestep-5Hz-lm-1.7B/*'])"

# Base DiT — the model LoRA trains on top of
RUN python -c "import os; from huggingface_hub import snapshot_download; snapshot_download('ACE-Step/acestep-v15-base', local_dir='/models/checkpoints/acestep-v15-base', token=os.environ.get('HF_TOKEN'))"

# -----------------------------------------------------------------------------
# Stage 2: Runtime
# cudnn-devel required for backprop during training
# -----------------------------------------------------------------------------
FROM nvidia/cuda:12.8.0-cudnn-devel-ubuntu22.04 AS runtime

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    DEBIAN_FRONTEND=noninteractive \
    CHECKPOINT_DIR=/app/checkpoints \
    DIT_MODEL=acestep-v15-base \
    DEVICE=cuda \
    LORA_OUTPUT_DIR=/runpod-volume/loras

WORKDIR /app

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-dev \
    python3-pip \
    git \
    curl \
    build-essential \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.11 /usr/bin/python3 \
    && ln -sf /usr/bin/python3.11 /usr/bin/python

RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.local/bin:$PATH"

RUN git clone https://github.com/ace-step/ACE-Step-1.5.git /app && \
    rm -rf /app/.git

RUN uv pip install --system --no-cache .

RUN uv pip install --system --no-cache \
    runpod \
    bitsandbytes \
    pytorch-lightning \
    torchmetrics

RUN ln -s /app/checkpoints /usr/local/lib/python3.11/dist-packages/checkpoints

# Copy baked-in checkpoints
COPY --from=model-downloader /models/checkpoints /app/checkpoints

# ── KEY FIX ──────────────────────────────────────────────────────────────────
# check_main_model_exists() looks for acestep-v15-turbo regardless of which
# DiT model we actually use. Without this placeholder it triggers a full
# re-download at runtime, fills the disk, and the job dies.
RUN mkdir -p /app/checkpoints/acestep-v15-turbo

RUN mkdir -p /app/outputs /tmp/training

COPY training_handler.py /app/training_handler.py

CMD ["python", "-u", "/app/training_handler.py"]